{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNfromScratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYJm+RINfraZhgzAEMfPF9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruv0x0x0/pclubsecytaskML/blob/main/CNNfromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eYDdI8345rC_",
        "outputId": "5ae3bcfd-ad69-4219-f0c1-ae7bae7d785e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.09998469 0.09999459 0.10001381 0.09998998 0.10001924 0.09999643\n",
            " 0.10001529 0.10000661 0.09998242 0.09999693]\n",
            "Epoch 1 ---->\n",
            "step\n",
            "0.01\n",
            "avg loss: \n",
            "0.0\n",
            "accuracy\n",
            "0\n",
            "\n",
            "\n",
            "step\n",
            "1.01\n",
            "avg loss: \n",
            "2.2887772123197436\n",
            "accuracy\n",
            "17\n",
            "\n",
            "\n",
            "step\n",
            "2.01\n",
            "avg loss: \n",
            "2.1732990259359894\n",
            "accuracy\n",
            "32\n",
            "\n",
            "\n",
            "step\n",
            "3.01\n",
            "avg loss: \n",
            "1.962889091822156\n",
            "accuracy\n",
            "40\n",
            "\n",
            "\n",
            "step\n",
            "4.01\n",
            "avg loss: \n",
            "1.4981951857432734\n",
            "accuracy\n",
            "54\n",
            "\n",
            "\n",
            "step\n",
            "5.01\n",
            "avg loss: \n",
            "1.342760403916806\n",
            "accuracy\n",
            "60\n",
            "\n",
            "\n",
            "step\n",
            "6.01\n",
            "avg loss: \n",
            "0.9638222490940754\n",
            "accuracy\n",
            "72\n",
            "\n",
            "\n",
            "step\n",
            "7.01\n",
            "avg loss: \n",
            "0.8561324323574508\n",
            "accuracy\n",
            "74\n",
            "\n",
            "\n",
            "step\n",
            "8.01\n",
            "avg loss: \n",
            "0.9075732464562275\n",
            "accuracy\n",
            "71\n",
            "\n",
            "\n",
            "step\n",
            "9.01\n",
            "avg loss: \n",
            "0.5960440102333628\n",
            "accuracy\n",
            "83\n",
            "\n",
            "\n",
            "step\n",
            "10.01\n",
            "avg loss: \n",
            "0.7338578662357784\n",
            "accuracy\n",
            "76\n",
            "\n",
            "\n",
            "step\n",
            "11.01\n",
            "avg loss: \n",
            "0.6847452067865418\n",
            "accuracy\n",
            "80\n",
            "\n",
            "\n",
            "step\n",
            "12.01\n",
            "avg loss: \n",
            "0.7563600377091325\n",
            "accuracy\n",
            "74\n",
            "\n",
            "\n",
            "step\n",
            "13.01\n",
            "avg loss: \n",
            "0.5271790819435048\n",
            "accuracy\n",
            "80\n",
            "\n",
            "\n",
            "step\n",
            "14.01\n",
            "avg loss: \n",
            "0.4863712969992675\n",
            "accuracy\n",
            "84\n",
            "\n",
            "\n",
            "step\n",
            "15.01\n",
            "avg loss: \n",
            "0.3736034200969239\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "16.01\n",
            "avg loss: \n",
            "0.6570969010791194\n",
            "accuracy\n",
            "77\n",
            "\n",
            "\n",
            "step\n",
            "17.01\n",
            "avg loss: \n",
            "0.5673079752210627\n",
            "accuracy\n",
            "83\n",
            "\n",
            "\n",
            "step\n",
            "18.01\n",
            "avg loss: \n",
            "0.4987854688268566\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "19.01\n",
            "avg loss: \n",
            "0.7519456822003058\n",
            "accuracy\n",
            "77\n",
            "\n",
            "\n",
            "step\n",
            "20.01\n",
            "avg loss: \n",
            "0.45822729308193744\n",
            "accuracy\n",
            "86\n",
            "\n",
            "\n",
            "step\n",
            "21.01\n",
            "avg loss: \n",
            "0.3875227942867643\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "22.01\n",
            "avg loss: \n",
            "0.4939044481629296\n",
            "accuracy\n",
            "86\n",
            "\n",
            "\n",
            "step\n",
            "23.01\n",
            "avg loss: \n",
            "0.4541834363425882\n",
            "accuracy\n",
            "86\n",
            "\n",
            "\n",
            "step\n",
            "24.01\n",
            "avg loss: \n",
            "0.4073618320638948\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "25.01\n",
            "avg loss: \n",
            "0.36862505008173263\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "26.01\n",
            "avg loss: \n",
            "0.7790918919735661\n",
            "accuracy\n",
            "77\n",
            "\n",
            "\n",
            "step\n",
            "27.01\n",
            "avg loss: \n",
            "0.4778698976304037\n",
            "accuracy\n",
            "84\n",
            "\n",
            "\n",
            "step\n",
            "28.01\n",
            "avg loss: \n",
            "0.4077660264621439\n",
            "accuracy\n",
            "87\n",
            "\n",
            "\n",
            "step\n",
            "29.01\n",
            "avg loss: \n",
            "0.3510589020508148\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "30.01\n",
            "avg loss: \n",
            "0.5186926370195666\n",
            "accuracy\n",
            "85\n",
            "\n",
            "\n",
            "step\n",
            "31.01\n",
            "avg loss: \n",
            "0.5401207237672034\n",
            "accuracy\n",
            "86\n",
            "\n",
            "\n",
            "step\n",
            "32.01\n",
            "avg loss: \n",
            "0.34223771970898725\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "33.01\n",
            "avg loss: \n",
            "0.3576493309902927\n",
            "accuracy\n",
            "86\n",
            "\n",
            "\n",
            "step\n",
            "34.01\n",
            "avg loss: \n",
            "0.4794612463525755\n",
            "accuracy\n",
            "87\n",
            "\n",
            "\n",
            "step\n",
            "35.01\n",
            "avg loss: \n",
            "0.3863307707986715\n",
            "accuracy\n",
            "86\n",
            "\n",
            "\n",
            "step\n",
            "36.01\n",
            "avg loss: \n",
            "0.5092694673211876\n",
            "accuracy\n",
            "83\n",
            "\n",
            "\n",
            "step\n",
            "37.01\n",
            "avg loss: \n",
            "0.2854021723987876\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "38.01\n",
            "avg loss: \n",
            "0.30041737689024745\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "39.01\n",
            "avg loss: \n",
            "0.4245699485060917\n",
            "accuracy\n",
            "86\n",
            "\n",
            "\n",
            "step\n",
            "40.01\n",
            "avg loss: \n",
            "0.5279329453702165\n",
            "accuracy\n",
            "83\n",
            "\n",
            "\n",
            "step\n",
            "41.01\n",
            "avg loss: \n",
            "0.38777054460106053\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "42.01\n",
            "avg loss: \n",
            "0.3169941570644352\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "43.01\n",
            "avg loss: \n",
            "0.35058787935485414\n",
            "accuracy\n",
            "87\n",
            "\n",
            "\n",
            "step\n",
            "44.01\n",
            "avg loss: \n",
            "0.5022736310262607\n",
            "accuracy\n",
            "84\n",
            "\n",
            "\n",
            "step\n",
            "45.01\n",
            "avg loss: \n",
            "0.45380966919371163\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "46.01\n",
            "avg loss: \n",
            "0.3334899978776202\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "47.01\n",
            "avg loss: \n",
            "0.5840027589514143\n",
            "accuracy\n",
            "85\n",
            "\n",
            "\n",
            "step\n",
            "48.01\n",
            "avg loss: \n",
            "0.35726458014208246\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "49.01\n",
            "avg loss: \n",
            "0.37692331773151444\n",
            "accuracy\n",
            "85\n",
            "\n",
            "\n",
            "step\n",
            "50.01\n",
            "avg loss: \n",
            "0.366112051083792\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "51.01\n",
            "avg loss: \n",
            "0.26808066304487943\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "52.01\n",
            "avg loss: \n",
            "0.3912754362027829\n",
            "accuracy\n",
            "87\n",
            "\n",
            "\n",
            "step\n",
            "53.01\n",
            "avg loss: \n",
            "0.2987755936629589\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "54.01\n",
            "avg loss: \n",
            "0.5850029663193549\n",
            "accuracy\n",
            "83\n",
            "\n",
            "\n",
            "step\n",
            "55.01\n",
            "avg loss: \n",
            "0.3012432207089338\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "56.01\n",
            "avg loss: \n",
            "0.30177946323177735\n",
            "accuracy\n",
            "96\n",
            "\n",
            "\n",
            "step\n",
            "57.01\n",
            "avg loss: \n",
            "0.509536336315322\n",
            "accuracy\n",
            "85\n",
            "\n",
            "\n",
            "step\n",
            "58.01\n",
            "avg loss: \n",
            "0.38846991001016673\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "59.01\n",
            "avg loss: \n",
            "0.39602458337841023\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "60.01\n",
            "avg loss: \n",
            "0.47993417121027415\n",
            "accuracy\n",
            "80\n",
            "\n",
            "\n",
            "step\n",
            "61.01\n",
            "avg loss: \n",
            "0.4728530737943366\n",
            "accuracy\n",
            "85\n",
            "\n",
            "\n",
            "step\n",
            "62.01\n",
            "avg loss: \n",
            "0.32154290813469805\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "63.01\n",
            "avg loss: \n",
            "0.3754735560761837\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "64.01\n",
            "avg loss: \n",
            "0.2635797787541797\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "65.01\n",
            "avg loss: \n",
            "0.33215221512101367\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "66.01\n",
            "avg loss: \n",
            "0.47860103921887664\n",
            "accuracy\n",
            "86\n",
            "\n",
            "\n",
            "step\n",
            "67.01\n",
            "avg loss: \n",
            "0.2510395666935463\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "68.01\n",
            "avg loss: \n",
            "0.44483861072743835\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "Epoch 2 ---->\n",
            "step\n",
            "0.01\n",
            "avg loss: \n",
            "0.0\n",
            "accuracy\n",
            "0\n",
            "\n",
            "\n",
            "step\n",
            "1.01\n",
            "avg loss: \n",
            "0.42311552370143524\n",
            "accuracy\n",
            "87\n",
            "\n",
            "\n",
            "step\n",
            "2.01\n",
            "avg loss: \n",
            "0.3457820542312984\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "3.01\n",
            "avg loss: \n",
            "0.30056620368434683\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "4.01\n",
            "avg loss: \n",
            "0.3605898196935828\n",
            "accuracy\n",
            "87\n",
            "\n",
            "\n",
            "step\n",
            "5.01\n",
            "avg loss: \n",
            "0.41985558595270334\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "6.01\n",
            "avg loss: \n",
            "0.3059607694487378\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "7.01\n",
            "avg loss: \n",
            "0.31715000453393033\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "8.01\n",
            "avg loss: \n",
            "0.21078130176619084\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "9.01\n",
            "avg loss: \n",
            "0.3620472339327588\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "10.01\n",
            "avg loss: \n",
            "0.4005032846052607\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "11.01\n",
            "avg loss: \n",
            "0.3536105415364508\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "12.01\n",
            "avg loss: \n",
            "0.5754656012511945\n",
            "accuracy\n",
            "87\n",
            "\n",
            "\n",
            "step\n",
            "13.01\n",
            "avg loss: \n",
            "0.33542803764917856\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "14.01\n",
            "avg loss: \n",
            "0.2693432119568899\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "15.01\n",
            "avg loss: \n",
            "0.4027619830085719\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "16.01\n",
            "avg loss: \n",
            "0.3268557487998527\n",
            "accuracy\n",
            "87\n",
            "\n",
            "\n",
            "step\n",
            "17.01\n",
            "avg loss: \n",
            "0.20604469026264535\n",
            "accuracy\n",
            "96\n",
            "\n",
            "\n",
            "step\n",
            "18.01\n",
            "avg loss: \n",
            "0.606877968371861\n",
            "accuracy\n",
            "81\n",
            "\n",
            "\n",
            "step\n",
            "19.01\n",
            "avg loss: \n",
            "0.2537448500501428\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "20.01\n",
            "avg loss: \n",
            "0.3122730030297383\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "21.01\n",
            "avg loss: \n",
            "0.28776211392604023\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "22.01\n",
            "avg loss: \n",
            "0.2882319192585681\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "23.01\n",
            "avg loss: \n",
            "0.46741064816320593\n",
            "accuracy\n",
            "85\n",
            "\n",
            "\n",
            "step\n",
            "24.01\n",
            "avg loss: \n",
            "0.36135356368308286\n",
            "accuracy\n",
            "87\n",
            "\n",
            "\n",
            "step\n",
            "25.01\n",
            "avg loss: \n",
            "0.29566572936593744\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "26.01\n",
            "avg loss: \n",
            "0.3962995144107267\n",
            "accuracy\n",
            "86\n",
            "\n",
            "\n",
            "step\n",
            "27.01\n",
            "avg loss: \n",
            "0.36554823064127384\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "28.01\n",
            "avg loss: \n",
            "0.30903984009439894\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "29.01\n",
            "avg loss: \n",
            "0.26049882984393125\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "30.01\n",
            "avg loss: \n",
            "0.31407348112797934\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "31.01\n",
            "avg loss: \n",
            "0.24396329728482466\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "32.01\n",
            "avg loss: \n",
            "0.41124663933881594\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "33.01\n",
            "avg loss: \n",
            "0.1782702168852883\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "34.01\n",
            "avg loss: \n",
            "0.3020558427883417\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "35.01\n",
            "avg loss: \n",
            "0.4144719659079351\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "36.01\n",
            "avg loss: \n",
            "0.5314883413550956\n",
            "accuracy\n",
            "83\n",
            "\n",
            "\n",
            "step\n",
            "37.01\n",
            "avg loss: \n",
            "0.37005958909586634\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "38.01\n",
            "avg loss: \n",
            "0.38954139410845806\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "39.01\n",
            "avg loss: \n",
            "0.44696000826067384\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "40.01\n",
            "avg loss: \n",
            "0.3067492251913854\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "41.01\n",
            "avg loss: \n",
            "0.2274083963653299\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "42.01\n",
            "avg loss: \n",
            "0.23279877045561562\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "43.01\n",
            "avg loss: \n",
            "0.26976492919117506\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "44.01\n",
            "avg loss: \n",
            "0.456152362958519\n",
            "accuracy\n",
            "87\n",
            "\n",
            "\n",
            "step\n",
            "45.01\n",
            "avg loss: \n",
            "0.26347279260652223\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "46.01\n",
            "avg loss: \n",
            "0.3232672320766296\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "47.01\n",
            "avg loss: \n",
            "0.2791800349595478\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "48.01\n",
            "avg loss: \n",
            "0.15523257109280464\n",
            "accuracy\n",
            "96\n",
            "\n",
            "\n",
            "step\n",
            "49.01\n",
            "avg loss: \n",
            "0.20194130948168032\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "50.01\n",
            "avg loss: \n",
            "0.35636853965934434\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "51.01\n",
            "avg loss: \n",
            "0.2674427783597855\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "52.01\n",
            "avg loss: \n",
            "0.27177974069656236\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "53.01\n",
            "avg loss: \n",
            "0.2057187377404218\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "54.01\n",
            "avg loss: \n",
            "0.27287399607337237\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "55.01\n",
            "avg loss: \n",
            "0.2602746143908329\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "56.01\n",
            "avg loss: \n",
            "0.3269438756892242\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "57.01\n",
            "avg loss: \n",
            "0.5168549510697649\n",
            "accuracy\n",
            "86\n",
            "\n",
            "\n",
            "step\n",
            "58.01\n",
            "avg loss: \n",
            "0.3295736065649728\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "59.01\n",
            "avg loss: \n",
            "0.3583933630150476\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "60.01\n",
            "avg loss: \n",
            "0.3949855072807105\n",
            "accuracy\n",
            "87\n",
            "\n",
            "\n",
            "step\n",
            "61.01\n",
            "avg loss: \n",
            "0.2277543196253234\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "62.01\n",
            "avg loss: \n",
            "0.2491763505122275\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "63.01\n",
            "avg loss: \n",
            "0.4225009979214609\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "64.01\n",
            "avg loss: \n",
            "0.21903548225302744\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "65.01\n",
            "avg loss: \n",
            "0.18361401619494847\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "66.01\n",
            "avg loss: \n",
            "0.31826458048754663\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "67.01\n",
            "avg loss: \n",
            "0.24884720558795773\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "68.01\n",
            "avg loss: \n",
            "0.25713625447790206\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "Epoch 3 ---->\n",
            "step\n",
            "0.01\n",
            "avg loss: \n",
            "0.0\n",
            "accuracy\n",
            "0\n",
            "\n",
            "\n",
            "step\n",
            "1.01\n",
            "avg loss: \n",
            "0.26279459648469905\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "2.01\n",
            "avg loss: \n",
            "0.13113401105658215\n",
            "accuracy\n",
            "97\n",
            "\n",
            "\n",
            "step\n",
            "3.01\n",
            "avg loss: \n",
            "0.3887787975156538\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "4.01\n",
            "avg loss: \n",
            "0.39817903605397065\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "5.01\n",
            "avg loss: \n",
            "0.2146560676639735\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "6.01\n",
            "avg loss: \n",
            "0.2902337078866317\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "7.01\n",
            "avg loss: \n",
            "0.2725743405974241\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "8.01\n",
            "avg loss: \n",
            "0.2653854454643285\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "9.01\n",
            "avg loss: \n",
            "0.2214850771408059\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "10.01\n",
            "avg loss: \n",
            "0.28845155204908296\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "11.01\n",
            "avg loss: \n",
            "0.31002255513174076\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "12.01\n",
            "avg loss: \n",
            "0.2500650910926433\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "13.01\n",
            "avg loss: \n",
            "0.21616392589072675\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "14.01\n",
            "avg loss: \n",
            "0.2384094653041107\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "15.01\n",
            "avg loss: \n",
            "0.28528621699299217\n",
            "accuracy\n",
            "87\n",
            "\n",
            "\n",
            "step\n",
            "16.01\n",
            "avg loss: \n",
            "0.1348014604443174\n",
            "accuracy\n",
            "96\n",
            "\n",
            "\n",
            "step\n",
            "17.01\n",
            "avg loss: \n",
            "0.18717425119998496\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "18.01\n",
            "avg loss: \n",
            "0.37402610745686504\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "19.01\n",
            "avg loss: \n",
            "0.3324209831753598\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "20.01\n",
            "avg loss: \n",
            "0.426271973167211\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "21.01\n",
            "avg loss: \n",
            "0.286133601665851\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "22.01\n",
            "avg loss: \n",
            "0.20459277730376838\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "23.01\n",
            "avg loss: \n",
            "0.27951662217456635\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "24.01\n",
            "avg loss: \n",
            "0.19272038276787337\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "25.01\n",
            "avg loss: \n",
            "0.22714122149129387\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "26.01\n",
            "avg loss: \n",
            "0.17086893682138185\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "27.01\n",
            "avg loss: \n",
            "0.4058388376720503\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "28.01\n",
            "avg loss: \n",
            "0.20769563198732702\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "29.01\n",
            "avg loss: \n",
            "0.4002475181288803\n",
            "accuracy\n",
            "86\n",
            "\n",
            "\n",
            "step\n",
            "30.01\n",
            "avg loss: \n",
            "0.30166558710832914\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "31.01\n",
            "avg loss: \n",
            "0.4826003128807742\n",
            "accuracy\n",
            "85\n",
            "\n",
            "\n",
            "step\n",
            "32.01\n",
            "avg loss: \n",
            "0.27272121996693566\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "33.01\n",
            "avg loss: \n",
            "0.17475517679914443\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "34.01\n",
            "avg loss: \n",
            "0.24769357060801575\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "35.01\n",
            "avg loss: \n",
            "0.34257121260151746\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "36.01\n",
            "avg loss: \n",
            "0.22932867625323272\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "37.01\n",
            "avg loss: \n",
            "0.18617472603574153\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "38.01\n",
            "avg loss: \n",
            "0.2482319177552049\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "39.01\n",
            "avg loss: \n",
            "0.254800262070366\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "40.01\n",
            "avg loss: \n",
            "0.2354571514837084\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "41.01\n",
            "avg loss: \n",
            "0.42054266975787036\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "42.01\n",
            "avg loss: \n",
            "0.32810361368053825\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "43.01\n",
            "avg loss: \n",
            "0.41214829202125003\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "44.01\n",
            "avg loss: \n",
            "0.30855260594637574\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "45.01\n",
            "avg loss: \n",
            "0.2780689173183375\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "46.01\n",
            "avg loss: \n",
            "0.1913179870622811\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "47.01\n",
            "avg loss: \n",
            "0.13254875478152872\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "48.01\n",
            "avg loss: \n",
            "0.2799689747015845\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "49.01\n",
            "avg loss: \n",
            "0.20794681878669408\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "50.01\n",
            "avg loss: \n",
            "0.23429088750465282\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "51.01\n",
            "avg loss: \n",
            "0.14157850870008123\n",
            "accuracy\n",
            "96\n",
            "\n",
            "\n",
            "step\n",
            "52.01\n",
            "avg loss: \n",
            "0.3614931130457202\n",
            "accuracy\n",
            "85\n",
            "\n",
            "\n",
            "step\n",
            "53.01\n",
            "avg loss: \n",
            "0.35921192061518015\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "54.01\n",
            "avg loss: \n",
            "0.3688684467787273\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "55.01\n",
            "avg loss: \n",
            "0.3832563818573692\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "56.01\n",
            "avg loss: \n",
            "0.21517891767889938\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "57.01\n",
            "avg loss: \n",
            "0.26996384833819864\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "58.01\n",
            "avg loss: \n",
            "0.5186162476920988\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "59.01\n",
            "avg loss: \n",
            "0.23259370377425345\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "60.01\n",
            "avg loss: \n",
            "0.12042448639133661\n",
            "accuracy\n",
            "98\n",
            "\n",
            "\n",
            "step\n",
            "61.01\n",
            "avg loss: \n",
            "0.336702383673764\n",
            "accuracy\n",
            "87\n",
            "\n",
            "\n",
            "step\n",
            "62.01\n",
            "avg loss: \n",
            "0.23251918352985063\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "63.01\n",
            "avg loss: \n",
            "0.3241061078115008\n",
            "accuracy\n",
            "87\n",
            "\n",
            "\n",
            "step\n",
            "64.01\n",
            "avg loss: \n",
            "0.22678369965647463\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "65.01\n",
            "avg loss: \n",
            "0.28042815871323395\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "66.01\n",
            "avg loss: \n",
            "0.25395745560185967\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "67.01\n",
            "avg loss: \n",
            "0.33250785386701026\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "68.01\n",
            "avg loss: \n",
            "0.2256718357278136\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "Epoch 4 ---->\n",
            "step\n",
            "0.01\n",
            "avg loss: \n",
            "0.0\n",
            "accuracy\n",
            "0\n",
            "\n",
            "\n",
            "step\n",
            "1.01\n",
            "avg loss: \n",
            "0.3608856545551373\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "2.01\n",
            "avg loss: \n",
            "0.20626345443917604\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "3.01\n",
            "avg loss: \n",
            "0.253555933844521\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "4.01\n",
            "avg loss: \n",
            "0.2700005183356635\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "5.01\n",
            "avg loss: \n",
            "0.18405492240300203\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "6.01\n",
            "avg loss: \n",
            "0.36421003376382843\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "7.01\n",
            "avg loss: \n",
            "0.23690973136519003\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "8.01\n",
            "avg loss: \n",
            "0.20251584072243056\n",
            "accuracy\n",
            "97\n",
            "\n",
            "\n",
            "step\n",
            "9.01\n",
            "avg loss: \n",
            "0.19882401202903086\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "10.01\n",
            "avg loss: \n",
            "0.28071867791547406\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "11.01\n",
            "avg loss: \n",
            "0.42160481180546017\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "12.01\n",
            "avg loss: \n",
            "0.20314436891652818\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "13.01\n",
            "avg loss: \n",
            "0.11431320320172399\n",
            "accuracy\n",
            "98\n",
            "\n",
            "\n",
            "step\n",
            "14.01\n",
            "avg loss: \n",
            "0.16028681059278302\n",
            "accuracy\n",
            "97\n",
            "\n",
            "\n",
            "step\n",
            "15.01\n",
            "avg loss: \n",
            "0.20046969881428733\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "16.01\n",
            "avg loss: \n",
            "0.27203087390608194\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "17.01\n",
            "avg loss: \n",
            "0.484669105334879\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "18.01\n",
            "avg loss: \n",
            "0.2991916345099795\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "19.01\n",
            "avg loss: \n",
            "0.4101074728005447\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "20.01\n",
            "avg loss: \n",
            "0.18784303304513986\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "21.01\n",
            "avg loss: \n",
            "0.3529907943406731\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "22.01\n",
            "avg loss: \n",
            "0.20389347842303318\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "23.01\n",
            "avg loss: \n",
            "0.193682840579768\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "24.01\n",
            "avg loss: \n",
            "0.29361427216501085\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "25.01\n",
            "avg loss: \n",
            "0.3370025241527614\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "26.01\n",
            "avg loss: \n",
            "0.28265911890690765\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "27.01\n",
            "avg loss: \n",
            "0.20046120981266458\n",
            "accuracy\n",
            "96\n",
            "\n",
            "\n",
            "step\n",
            "28.01\n",
            "avg loss: \n",
            "0.3363237529626831\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "29.01\n",
            "avg loss: \n",
            "0.2573153391645193\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "30.01\n",
            "avg loss: \n",
            "0.19939742170997019\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "31.01\n",
            "avg loss: \n",
            "0.23680796472584226\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "32.01\n",
            "avg loss: \n",
            "0.14091891769292006\n",
            "accuracy\n",
            "97\n",
            "\n",
            "\n",
            "step\n",
            "33.01\n",
            "avg loss: \n",
            "0.2440146497384932\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "34.01\n",
            "avg loss: \n",
            "0.2804351584597367\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "35.01\n",
            "avg loss: \n",
            "0.23466665894877592\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "36.01\n",
            "avg loss: \n",
            "0.28436916396464357\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "37.01\n",
            "avg loss: \n",
            "0.21669877487228587\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "38.01\n",
            "avg loss: \n",
            "0.22767496480482388\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "39.01\n",
            "avg loss: \n",
            "0.25082409961905705\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "40.01\n",
            "avg loss: \n",
            "0.21696974756952542\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "41.01\n",
            "avg loss: \n",
            "0.3053986204226654\n",
            "accuracy\n",
            "91\n",
            "\n",
            "\n",
            "step\n",
            "42.01\n",
            "avg loss: \n",
            "0.3054407355280948\n",
            "accuracy\n",
            "89\n",
            "\n",
            "\n",
            "step\n",
            "43.01\n",
            "avg loss: \n",
            "0.24411328893454842\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "44.01\n",
            "avg loss: \n",
            "0.211321033705736\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "45.01\n",
            "avg loss: \n",
            "0.24556981544546486\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "46.01\n",
            "avg loss: \n",
            "0.2441829993164143\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "47.01\n",
            "avg loss: \n",
            "0.25669565078402323\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "48.01\n",
            "avg loss: \n",
            "0.14344340790039756\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "49.01\n",
            "avg loss: \n",
            "0.17245824431945564\n",
            "accuracy\n",
            "96\n",
            "\n",
            "\n",
            "step\n",
            "50.01\n",
            "avg loss: \n",
            "0.12984043950375596\n",
            "accuracy\n",
            "97\n",
            "\n",
            "\n",
            "step\n",
            "51.01\n",
            "avg loss: \n",
            "0.31879743810757344\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "52.01\n",
            "avg loss: \n",
            "0.18311660701939517\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "53.01\n",
            "avg loss: \n",
            "0.1407257426522442\n",
            "accuracy\n",
            "96\n",
            "\n",
            "\n",
            "step\n",
            "54.01\n",
            "avg loss: \n",
            "0.35759589598543046\n",
            "accuracy\n",
            "87\n",
            "\n",
            "\n",
            "step\n",
            "55.01\n",
            "avg loss: \n",
            "0.12771650898682707\n",
            "accuracy\n",
            "98\n",
            "\n",
            "\n",
            "step\n",
            "56.01\n",
            "avg loss: \n",
            "0.25697373416242764\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "57.01\n",
            "avg loss: \n",
            "0.11438621174970662\n",
            "accuracy\n",
            "98\n",
            "\n",
            "\n",
            "step\n",
            "58.01\n",
            "avg loss: \n",
            "0.10172944568701596\n",
            "accuracy\n",
            "97\n",
            "\n",
            "\n",
            "step\n",
            "59.01\n",
            "avg loss: \n",
            "0.20926080582618303\n",
            "accuracy\n",
            "97\n",
            "\n",
            "\n",
            "step\n",
            "60.01\n",
            "avg loss: \n",
            "0.2835525711042904\n",
            "accuracy\n",
            "88\n",
            "\n",
            "\n",
            "step\n",
            "61.01\n",
            "avg loss: \n",
            "0.1319269422625864\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "62.01\n",
            "avg loss: \n",
            "0.24072664879754238\n",
            "accuracy\n",
            "94\n",
            "\n",
            "\n",
            "step\n",
            "63.01\n",
            "avg loss: \n",
            "0.2669006083250835\n",
            "accuracy\n",
            "90\n",
            "\n",
            "\n",
            "step\n",
            "64.01\n",
            "avg loss: \n",
            "0.15815905701796745\n",
            "accuracy\n",
            "95\n",
            "\n",
            "\n",
            "step\n",
            "65.01\n",
            "avg loss: \n",
            "0.3254412436288648\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "66.01\n",
            "avg loss: \n",
            "0.25541531946198914\n",
            "accuracy\n",
            "92\n",
            "\n",
            "\n",
            "step\n",
            "67.01\n",
            "avg loss: \n",
            "0.24785795910408034\n",
            "accuracy\n",
            "93\n",
            "\n",
            "\n",
            "step\n",
            "68.01\n",
            "avg loss: \n",
            "0.06319209841414201\n",
            "accuracy\n",
            "96\n",
            "\n",
            "\n",
            "*TestingPhase*\n",
            "Loss: 0.3078224674764611\n",
            "Accuracy: 0.908840579710145\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n",
            "image is probably 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n",
            "image is probably 7\n"
          ]
        }
      ],
      "source": [
        "from matplotlib import image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from numpy import asarray\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "(X_train, Y_train), (X_test,Y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Filter Array size-nxn, conv filter nfxnxn\n",
        "class Convulation:\n",
        "    def __init__(self, nf, n):\n",
        "        self.nf = nf\n",
        "        self.n = n\n",
        "        self.filter = np.random.randn(nf, n, n)/(n*n) #will initial a normalized 3D matrix with random numbers\n",
        "\n",
        "    def patch(self, img):\n",
        "        h, w= img.shape\n",
        "        self.img = img\n",
        "        for i in range(h-self.n+1):\n",
        "          for j in range(w-self.n+1):\n",
        "            img_patch=img[i:i+self.n,j:j+self.n]\n",
        "            yield img_patch, i, j\n",
        "\n",
        "    def fwdprop(self, img):\n",
        "        h, w= img.shape  \n",
        "        Convulation_output = np.zeros((h-self.n + 1, w - self.n + 1, self.nf))\n",
        "        for img_patch, i, j in self.patch(img):\n",
        "            Convulation_output[i, j] = np.sum(img_patch*self.filter, axis=(1, 2))\n",
        "        return Convulation_output\n",
        "\n",
        "    def backprop(self, dL_dout, lrate):\n",
        "        dl_dF_params = np.zeros(self.filter.shape)\n",
        "        for img_patch, i, j in self.patch(self.img):\n",
        "            for k in range(self.n):\n",
        "                dl_dF_params[k] += img_patch*dL_dout[i, j, k]\n",
        "        self.filter -= lrate*dl_dF_params\n",
        "        return dl_dF_params    \n",
        "\"\"\"      \n",
        "conn = Convulation(18, 7)\n",
        "out = conn.fwdprop(img)\n",
        "print(out.shape)\n",
        "plt.imshow(out[:, :, 17], cmap='gray')\n",
        "plt.show()\n",
        "\"\"\"\n",
        "class Max_Pool:\n",
        "    def __init__(self, n):\n",
        "         self.n=n\n",
        "    def patch(self, img):     \n",
        "      h2= img.shape[0] // self.n\n",
        "      w2= img.shape[1] // self.n\n",
        "      self.img=img\n",
        "      \n",
        "      for i in range(h2):\n",
        "        for j in range(w2):\n",
        "          img_patch=img[(i*self.n): (i*self.n+self.n),(j*self.n): (j*self.n+self.n),]\n",
        "          yield img_patch, i, j\n",
        "\n",
        "    def fwd_prop(self,img):\n",
        "      h, w, nf=img.shape\n",
        "      output=np.zeros((h // self.n, w // self.n, nf)) \n",
        "      for img_patch,i,j in self.patch(img):\n",
        "        output[i,j]=np.amax(img_patch,axis=(0,1))\n",
        "      return output\n",
        "\n",
        "    def back_prop(self, dL_dout):\n",
        "      dL_dmax_pool=np.zeros(self.img.shape)\n",
        "      for img_patch, i, j, in self.patch(self.img):\n",
        "        h, w, nf=img_patch.shape\n",
        "        maximum_val=np.amax(img_patch,axis=(0,1))\n",
        "\n",
        "        for i1 in range(h):\n",
        "         for j1 in range(w):\n",
        "           for k1 in range(nf):\n",
        "             if img_patch[i1, j1, k1]== maximum_val[k1]:\n",
        "               dL_dmax_pool[i*self.n+i1, j*self.n+j1, k1]=dL_dout[i,j,k1]\n",
        "      return dL_dmax_pool    \n",
        "\n",
        "\n",
        "\"\"\"\n",
        "conn2= Max_Pool(4)\n",
        "out2= conn2.fwd_prop(out)\n",
        "print(out2.shape)\n",
        "plt.imshow(out2[:, :, 17], cmap='gray')\n",
        "plt.show()  \n",
        "\"\"\"\n",
        "class Softmax:\n",
        "  def __init__(self, input_node, softmax_node):\n",
        "    self.weight=np.random.randn(input_node,softmax_node)/input_node\n",
        "    self.bias=np.zeros(softmax_node)\n",
        "\n",
        "  def forward_prop(self,image):\n",
        "    self.orig_im_shape=image.shape\n",
        "    image_modified=image.flatten()\n",
        "    self.modified_input=image_modified\n",
        "    output_val = np.dot(image_modified, self.weight) + self.bias\n",
        "    self.out=output_val\n",
        "    exp_out=np.exp(output_val)\n",
        "    return exp_out/np.sum(exp_out, axis=0)\n",
        "\n",
        "  def back_prop(self, dL_dout, learning_rate):\n",
        "    for i,grad in enumerate(dL_dout):\n",
        "      if grad==0:\n",
        "        continue\n",
        "\n",
        "      transformation_eq=np.exp(self.out)\n",
        "      S_total=np.sum(transformation_eq)\n",
        "\n",
        "      dy_dz=-transformation_eq[i]* transformation_eq/(S_total**2)\n",
        "      dy_dz[i]=transformation_eq[i]*(S_total-transformation_eq[i])/(S_total**2)\n",
        "\n",
        "      dz_dw=self.modified_input\n",
        "      dz_db=1\n",
        "      dz_d_inp=self.weight\n",
        "      dL_dz=grad*dy_dz\n",
        "      dL_dw=dz_dw[np.newaxis].T @ dL_dz[np.newaxis]\n",
        "      dL_db=dL_dz*dz_db\n",
        "      dL_d_inp=dz_d_inp @ dL_dz\n",
        "      self.weight-= learning_rate* dL_dw      \n",
        "      self.bias-= learning_rate* dL_db     \n",
        "\n",
        "      return dL_d_inp.reshape(self.orig_im_shape) \n",
        "class Softmax:\n",
        "  def __init__(self, input_node, softmax_node):\n",
        "    self.weight=np.random.randn(input_node,softmax_node)/input_node\n",
        "    self.bias=np.zeros(softmax_node)\n",
        "\n",
        "  def fwd_prop(self,image):\n",
        "    self.orig_im_shape=image.shape\n",
        "    image_modified=image.flatten()\n",
        "    self.modified_input=image_modified\n",
        "    output_val = np.dot(image_modified, self.weight) + self.bias\n",
        "    self.out=output_val\n",
        "    exp_out=np.exp(output_val)\n",
        "    return exp_out/np.sum(exp_out, axis=0)\n",
        "\n",
        "  def back_prop(self, dL_dout, learning_rate):\n",
        "    for i,grad in enumerate(dL_dout):\n",
        "      if grad==0:\n",
        "        continue\n",
        "\n",
        "      transformation_eq=np.exp(self.out)\n",
        "      S_total=np.sum(transformation_eq)\n",
        "\n",
        "      dy_dz=-transformation_eq[i]* transformation_eq/(S_total**2)\n",
        "      dy_dz[i]=transformation_eq[i]*(S_total-transformation_eq[i])/(S_total**2)\n",
        "\n",
        "      dz_dw=self.modified_input\n",
        "      dz_db=1\n",
        "      dz_d_inp=self.weight\n",
        "      dL_dz=grad*dy_dz\n",
        "      dL_dw=dz_dw[np.newaxis].T @ dL_dz[np.newaxis]\n",
        "      dL_db=dL_dz*dz_db\n",
        "      dL_d_inp=dz_d_inp @ dL_dz\n",
        "      self.weight-= learning_rate* dL_dw      \n",
        "      self.bias-= learning_rate* dL_db     \n",
        "\n",
        "      return dL_d_inp.reshape(self.orig_im_shape) \n",
        "\n",
        "conn3=Softmax(18*393*268, 10)\n",
        "out3=conn3.fwd_prop(out2)\n",
        "print(out3)\n",
        "x=6900\n",
        "train_images= X_train[:x]\n",
        "train_labels= Y_train[:x]\n",
        "test_images= X_test[:x]\n",
        "test_labels= Y_test[:x]\n",
        "conv=Convulation(8,3)\n",
        "pool=Max_Pool(2)\n",
        "softmax=Softmax(13*13*8,10)\n",
        "\n",
        "def cnn_forward_prop(image, label):\n",
        "  out_p= conv.fwdprop((image/255)-0.5)\n",
        "  out_p= pool.fwd_prop(out_p)\n",
        "  out_p=softmax.fwd_prop(out_p)\n",
        "  cross_ent_loss=-np.log(out_p[label]) if out_p[label]>0 else 0\n",
        "  accuracy_eval=1 if np.argmax(out_p) == label else 0\n",
        "  return out_p, cross_ent_loss, accuracy_eval\n",
        "\n",
        "def training_cnn(image, label, learn_rate=0.005):\n",
        "  out, loss, acc= cnn_forward_prop(image,label) \n",
        "  gradient= np.zeros(10)\n",
        "  gradient[label]=-1/out[label]\n",
        "\n",
        "  grad_back=softmax.back_prop(gradient, learn_rate)\n",
        "  grad_back= pool.back_prop(grad_back)\n",
        "  grad_back= conv.backprop(grad_back,learn_rate)\n",
        "\n",
        "  return loss, acc  \n",
        "for epoch1 in range(4):\n",
        "  print('Epoch %d ---->' % (epoch1+1))\n",
        "\n",
        "  shuffle_data=np.random.permutation(len(train_images))\n",
        "  train_images=train_images[shuffle_data]\n",
        "  train_labels=train_labels[shuffle_data]\n",
        "\n",
        "  loss=0\n",
        "  num_correct=0\n",
        "  for i, (im,label) in enumerate(zip(train_images,train_labels)):\n",
        "    if i % 100 ==0:\n",
        "    \n",
        "      print('step')\n",
        "      print((i+1)/100)\n",
        "      print('avg loss: ')\n",
        "      print(loss/100)\n",
        "      print('accuracy')\n",
        "      print(num_correct)\n",
        "      print('\\n')\n",
        "      loss=0\n",
        "      num_correct=0\n",
        "    l1,accu=training_cnn(im, label)\n",
        "    loss+=l1\n",
        "    num_correct+=accu\n",
        "print('*TestingPhase*')\n",
        "loss=0\n",
        "num_correct=0\n",
        "for im,label in zip(test_images,test_labels):\n",
        "  _, l1, accu=cnn_forward_prop(im, label)\n",
        "  loss+=l1\n",
        "  num_correct+=accu\n",
        "num_tests=len(test_images)\n",
        "\n",
        "print('Loss:', loss/num_tests) \n",
        "print('Accuracy:',num_correct/num_tests)\n",
        "\n",
        "\n",
        "y=69\n",
        "plt.imshow(train_images[y], cmap='gray')\n",
        "plt.show()\n",
        "print(train_images[y].shape)\n",
        "out_p= conv.fwdprop((train_images[y]/255)-0.5)\n",
        "out_p= pool.fwd_prop(out_p)\n",
        "out_p=softmax.fwd_prop(out_p)\n",
        "print(\"image is probably\",np.argmax(out_p))\n",
        "\"\"\"\n",
        "img = cv2.imread('/content/sample_data/5.png', cv2.IMREAD_GRAYSCALE)/255\n",
        "def predict(img1):\n",
        "  img = cv2.resize(img1, (28, 28))\n",
        "  plt.imshow(img, cmap='gray')\n",
        "  plt.show()\n",
        "  print(img.shape)\n",
        "  out_p= conv.fwdprop((img/255)-0.5)\n",
        "  out_p= pool.fwd_prop(out_p)\n",
        "  out_p=softmax.fwd_prop(out_p)\n",
        "  print(\"image is probably\",np.argmax(out_p))\n",
        "predict(img)\n",
        "\"\"\""
      ]
    }
  ]
}